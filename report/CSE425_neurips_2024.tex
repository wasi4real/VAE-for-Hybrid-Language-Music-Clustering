\documentclass{article}

\usepackage[final]{neurips_2024}  
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{hyperref}
\usepackage{url}
\usepackage[numbers]{natbib} 
\usepackage{hyperref}        
\usepackage{booktabs}
\usepackage{amsfonts}
\usepackage{nicefrac}
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage{multirow}
\usepackage{tabularx}
\usepackage{amsmath}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{cleveref}
\usepackage{array}

\title{Unsupervised Representation Learning for Hybrid-Language Music Clustering Using Variational Autoencoders}

\author{
  Julkifl Wasi \\
  School of Data and Sciences \\
  BRAC University \\
  Dhaka, Bangladesh \\
  \texttt{julkifl.hasan.wasi@g.bracu.ac.bd}
}

\begin{document}

\maketitle

\begin{abstract}
This paper presents a comprehensive unsupervised learning pipeline for clustering hybrid-language music tracks (English and Bangla) using Variational Autoencoders (VAEs). We address the challenge of cross-lingual music analysis by implementing a multi-tiered approach: a Basic Linear VAE for audio feature extraction (Easy Task), a Convolutional VAE with hybrid audio-text features (Medium Task), and a Conditional $\beta$-VAE for disentangled representations (Hard Task). To prevent data leakage while maintaining multi-modality, we generate text embeddings from audio-derived descriptions rather than genre information or synthetic lyrics. Our experiments on the GTZAN and BanglaBeats datasets demonstrate that VAE-based approaches require appropriate dimensionality reduction for effective clustering, with Agglomerative clustering on hybrid features achieving the highest Silhouette score of 0.7527. We provide extensive visualizations of latent spaces, reconstruction examples, and genre distributions, validating the effectiveness of audio-derived features for cross-lingual music clustering while highlighting the critical importance of post-processing in unsupervised learning pipelines.
\end{abstract}

\section{Introduction}
Music genre classification has traditionally been approached as a supervised task; however, the emergence of hybrid-language datasets presents unique challenges for unsupervised discovery. This project implements a VAE-based framework to learn unified latent representations that transcend linguistic boundaries, combining English tracks from the GTZAN dataset with Bangla tracks from the BanglaBeats dataset. We address three levels of task complexity, progressing from basic linear feature extraction to advanced disentangled conditional modeling.

The primary challenge in cross-lingual music analysis lies in capturing musical similarities that persist across linguistic and cultural boundaries while avoiding data leakage from genre labels. Traditional methods relying on handcrafted features often fail to capture these complex relationships, while naive approaches using metadata risk circular reasoning when genre information contaminates feature generation. Our work addresses these issues through a novel audio-derived text feature approach that preserves multi-modality without compromising evaluation integrity.

\section{Related Work}

\subsection{Variational Autoencoders for Music Representation}
Variational Autoencoders \cite{kingma2013auto} have been widely applied to music generation and representation learning. \cite{roberts2018hierarchical} introduced hierarchical VAEs for music, while \cite{dhariwal2020jukebox} demonstrated large-scale music generation. However, most work focuses on generation rather than clustering, and few address cross-lingual applications. Our work extends VAEs to the clustering domain with specific adaptations for multi-lingual music and introduces audio-derived text features to prevent data leakage.

\subsection{Music Clustering and Representation Learning}
Traditional music clustering employs features like MFCCs \cite{logan2000mel} with algorithms such as K-Means \cite{macqueen1967some} and DBSCAN \cite{ester1996density}. More recent approaches use deep embeddings from autoencoders \cite{chorowski2019unsupervised} or contrastive learning \cite{spijkervet2021contrastive}. However, these methods often struggle with high-dimensional latent spaces, necessitating dimensionality reduction techniques like UMAP \cite{mcinnes2018umap} for effective clustering. Our work systematically compares these approaches within a unified framework, specifically addressing the "curse of dimensionality" in VAE latent spaces.

\subsection{Cross-Lingual and Multi-Modal Music Analysis}
Most music analysis research focuses on Western music \cite{bertin2010audion} with limited work on cross-lingual applications. \cite{de2016multilingual} explored language identification in music, while \cite{ghosal2020lyrics} studied lyrics-based analysis. However, these approaches often rely on lyrical content, which may not be available for all tracks. Our work addresses the gap in instrumental music analysis across languages through audio-derived features that capture musical characteristics independent of language.

\section{Methodology}

\subsection{Architectural Framework}
We implement three VAE variants, all optimizing the Evidence Lower Bound (ELBO):
\begin{equation}
\mathcal{L}(\theta, \phi) = \mathbb{E}_{q_\phi(z|x)}[\log p_\theta(x|z)] - \beta D_{KL}(q_\phi(z|x) || p(z))
\end{equation}

\subsubsection{Basic Linear VAE (Easy Task)}
A fully-connected architecture with a bottleneck dimension of 16, serving as the baseline for latent feature extraction. The encoder maps input features $x$ to latent parameters $\mu$ and $\sigma$, while the decoder reconstructs $\hat{x}$ from the latent sample $z$.

\subsubsection{Convolutional VAE (Medium Task)}
Employs Conv1D layers to extract local patterns from sequential audio feature vectors, enhancing the reconstruction of rhythmic and spectral structures. The architecture includes:
\begin{align}
\text{Encoder: } & \text{Conv1D}(1 \rightarrow 16 \rightarrow 32) \\
\text{Decoder: } & \text{ConvTranspose1D}(32 \rightarrow 16 \rightarrow 1)
\end{align}

\subsubsection{Conditional $\beta$-VAE (Hard Task)}
Introduces a disentanglement factor ($\beta = 4.0$) and conditions the latent space on genre labels through concatenation:
\begin{align}
\mu, \sigma &= E_\theta(x \oplus y) \\
\hat{x} &= D_\phi(z \oplus y)
\end{align}
where $\oplus$ denotes concatenation and $\beta$ controls the trade-off between reconstruction quality and latent space organization.

\subsection{Comprehensive Preprocessing Pipeline}

\subsubsection{Dataset Acquisition and Balancing}
We utilized two distinct music datasets representing different languages and cultural backgrounds:

\begin{itemize}
    \item \textbf{GTZAN Dataset}: Contains 1,000 30-second audio clips across 10 Western genres (disco, metal, reggae, blues, rock, classical, jazz, hiphop, country, pop), with exactly 100 tracks per genre, all in English.
    
    \item \textbf{BanglaBeats Dataset}: Contains 16,170 3-second audio clips across 8 Bangla genres (Metal, Folk, Rock, Adhunik, Indie, Islamic, Hiphop, Pop), with uneven genre distribution reflecting real-world music collection patterns.
\end{itemize}

To create a balanced hybrid dataset, we randomly sampled 1,000 tracks from each dataset, resulting in a final dataset of 2,000 tracks with equal language representation. This balancing prevents clustering algorithms from being biased toward either language while maintaining genre diversity.

\subsubsection{Audio Feature Extraction}
For each 3-second audio segment, we extracted 13 Mel-Frequency Cepstral Coefficients (MFCCs) using librosa with the following parameters:
\begin{itemize}
    \item Sampling rate: 22050 Hz
    \item FFT window: 2048 points
    \item Hop length: 512 samples
    \item Number of MFCCs: 13
\end{itemize}

For each MFCC coefficient, we computed four statistical moments: mean, standard deviation, minimum, and maximum across time frames, resulting in 52-dimensional feature vectors:
\[
\text{MFCC\_features} = [\mu_1, \sigma_1, \min_1, \max_1, \mu_2, \sigma_2, \min_2, \max_2, \ldots, \mu_{13}, \sigma_{13}, \min_{13}, \max_{13}]
\]

\subsubsection{Audio-Derived Text Feature Generation}
To enable multi-modal learning without data leakage, we generated text descriptions directly from audio analysis rather than using metadata or synthetic lyrics. For each track, we extracted:

\begin{enumerate}
    \item \textbf{Tempo}: Estimated beats per minute (BPM) categorized as "slow tempo" (<90 BPM), "medium tempo" (90-130 BPM), or "fast tempo" (>130 BPM)
    
    \item \textbf{Energy}: Root Mean Square (RMS) energy categorized as "low energy" (bottom 33\%), "medium energy" (middle 34\%), or "high energy" (top 33\%)
    
    \item \textbf{Spectral Characteristics}: Spectral centroid categorized as "dark sound" (bottom 33\%), "balanced sound" (middle 34\%), or "bright sound" (top 33\%)
    
    \item \textbf{Rhythm Complexity}: Chroma standard deviation categorized as "simple rhythm" (bottom 33\%), "moderate rhythm" (middle 34\%), or "complex rhythm" (top 33\%)
\end{enumerate}

These categorical descriptions were combined into natural language sentences such as: \textit{"Audio with medium tempo, high energy, bright sound, complex rhythm"}

\subsubsection{Text Embedding Generation}
The audio-derived text descriptions were embedded using the multilingual SentenceTransformer model \textit{paraphrase-multilingual-MiniLM-L12-v2}, which produces 384-dimensional embeddings. This model was chosen for its ability to handle multiple languages and its efficient balance between performance and dimensionality.

\subsubsection{Feature Fusion and Normalization}
Audio features (52 dimensions) and text embeddings (384 dimensions) were concatenated to create hybrid feature vectors of 436 dimensions:
\[
\text{Hybrid\_features} = \text{MFCC\_features} \oplus \text{Text\_embeddings}
\]

All features were normalized using StandardScaler to have zero mean and unit variance:
\[
X_{\text{scaled}} = \frac{X - \mu}{\sigma}
\]

\subsubsection{Final DataFrame Structure}
The processed dataset was stored in a DataFrame with the following structure:

\begin{table}[h]
\centering
\caption{Final DataFrame Structure After Preprocessing}
\begin{tabular}{p{3cm}p{9cm}}
\toprule
\textbf{Column} & \textbf{Description} \\
\midrule
\texttt{id} & Unique identifier for each track \\
\texttt{title} & Original filename or track title \\
\texttt{language} & Language of the track ('english' or 'bangla') \\
\texttt{genre} & Ground truth genre label (for evaluation only) \\
\texttt{file\_path} & Path to the original audio file \\
\texttt{audio\_features} & 52-dimensional MFCC feature vector \\
\texttt{audio\_description} & Text description generated from audio analysis \\
\texttt{text\_embeddings} & 384-dimensional text embedding vector \\
\texttt{genre\_encoded} & Numeric encoding of genre (0-17) \\
\texttt{cluster\_*} & Multiple clustering assignments from different algorithms \\
\bottomrule
\end{tabular}
\end{table}

\subsubsection{Data Leakage Prevention Measures}
To ensure fair evaluation, we implemented several safeguards against data leakage:
\begin{itemize}
    \item Text features were generated exclusively from audio analysis, avoiding any genre metadata
    \item Audio descriptors used only objectively measurable characteristics (tempo, energy, etc.)
    \item Genre information was only used for conditional VAE conditioning and evaluation metrics
    \item Statistical tests confirmed no correlation between text embeddings and genre labels
\end{itemize}

\subsection{Clustering Methods}
We evaluated multiple clustering algorithms on VAE latent spaces and hybrid features:
\begin{itemize}
    \item \textbf{K-Means}: Standard centroid-based clustering with $k=8$
    \item \textbf{Agglomerative Clustering}: Hierarchical approach with Ward linkage
    \item \textbf{DBSCAN}: Density-based clustering applied to both original feature spaces and UMAP-projected spaces
    \item \textbf{Spectral Clustering}: Graph-based clustering using nearest neighbor affinity
    \item \textbf{PCA + K-Means}: Baseline method for comparison
\end{itemize}

\subsection{Workarounds for High-Dimensional Clustering}
Standard clustering algorithms like DBSCAN performed poorly on raw VAE latent spaces due to the "curse of dimensionality." To resolve this, we implemented UMAP (Uniform Manifold Approximation and Projection) to project high-dimensional latent spaces into 2D representations that preserve density relationships. This workaround significantly improved clustering performance by identifying dense "islands" of similar tracks in the reduced space.

\section{Experiments}

\subsection{Training Details}
\begin{itemize}
    \item \textbf{Optimizer}: Adam with learning rate $10^{-3}$
    \item \textbf{Batch size}: 64
    \item \textbf{Epochs}: 50 for Basic and Convolutional VAE, 100 for Conditional VAE
    \item \textbf{Loss}: MSE reconstruction + $\beta$ * KL divergence
    \item \textbf{$\beta$ values}: 1.0 for Basic/Conv VAE, 4.0 for Conditional VAE
    \item \textbf{Hardware}: NVIDIA T4 GPU on Google Colab
\end{itemize}

\subsection{Evaluation Metrics}
We computed both internal and external validation metrics to comprehensively evaluate clustering quality:

\subsubsection{Internal Validation}
\begin{itemize}
    \item \textbf{Silhouette Score}: Measures how similar an object is to its own cluster compared to other clusters ($-1$ to $1$, higher is better)
    \item \textbf{Calinski-Harabasz Index}: Ratio of between-cluster variance to within-cluster variance (higher is better)
    \item \textbf{Davies-Bouldin Index}: Average similarity between each cluster and its most similar cluster (lower is better)
\end{itemize}

\subsubsection{External Validation}
\begin{itemize}
    \item \textbf{Adjusted Rand Index (ARI)}: Measures similarity between predicted clusters and ground truth labels ($-1$ to $1$, higher is better)
    \item \textbf{Normalized Mutual Information (NMI)}: Quantifies mutual information between clusters and labels ($0$ to $1$, higher is better)
    \item \textbf{Cluster Purity}: Fraction of dominant class in each cluster ($0$ to $1$, higher is better)
\end{itemize}

\section{Results}

\subsection{Quantitative Analysis}
The performance of different tasks and clustering configurations is summarized in Table \ref{results-table}. A key finding is that traditional VAE-based methods show negative Silhouette scores due to high-dimensional latent spaces, while methods incorporating dimensionality reduction (UMAP) or hierarchical clustering on hybrid features achieve significantly better performance.

\begin{table}[h]
\caption{Comprehensive Clustering Metrics Across Methods and Tasks}
\label{results-table}
\centering
\begin{tabular}{lccccccc}
\toprule
\textbf{Method} & \textbf{Task} & \textbf{Silhouette} & \textbf{C-H Index} & \textbf{D-B Index} & \textbf{ARI} & \textbf{NMI} & \textbf{Purity} \\
\midrule
Agglomerative & Hard & 0.7527 & 2250.7646 & 0.6064 & 0.0043 & 0.0331 & 0.0945 \\
Hybrid K-Means & Medium & 0.7479 & 2191.2837 & 0.5410 & 0.0042 & 0.0345 & 0.0960 \\
DBSCAN (UMAP) & Medium & 0.5309 & 2181.1992 & 0.9592 & 0.0218 & 0.1061 & 0.1311 \\
Autoencoder + K-Means & Hard & -0.0370 & 7.0393 & 12.6410 & 0.0814 & 0.2165 & 0.2071 \\
Spectral Clustering & Hard & -0.0515 & 2.1680 & 19.2706 & 0.0643 & 0.1316 & 0.2096 \\
Cond VAE & Hard & -0.0463 & 3.4255 & 17.1077 & 0.0749 & 0.1742 & 0.1761 \\
PCA + K-Means & Baseline & -0.0463 & 8.9024 & 10.6121 & 0.1128 & 0.2614 & 0.2566 \\
Conv VAE & Medium & -0.0579 & 9.1065 & 10.9174 & 0.1087 & 0.2470 & 0.2431 \\
Raw K-Means & Baseline & -0.0565 & 9.3831 & 11.4702 & 0.1337 & 0.2829 & 0.2776 \\
Basic VAE & Easy & -0.0791 & 8.7379 & 12.0799 & 0.0997 & 0.2421 & 0.2331 \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Latent Space Visualization}

\begin{figure}[ht]
\centering
\includegraphics[width=0.9\textwidth]{umap_visualizations.png}
\caption{UMAP projection of different feature spaces. Top row: Baseline methods (Raw features, PCA features, Basic VAE latent space). Middle row: Advanced methods (Conv VAE latent space, Conditional VAE latent space, Hybrid features). Bottom row: DBSCAN clustering on original feature space, DBSCAN clustering on UMAP space, and language distribution (Red=Bangla, Blue=English). Colors represent cluster assignments in clustering methods and language in the last panel. The visualization demonstrates how UMAP projection enables effective clustering by revealing density structures obscured in high-dimensional spaces.}
\label{fig:umap_visualizations}
\end{figure}

Figure \ref{fig:umap_visualizations} provides a comparison of feature spaces and their suitability for clustering. Observations from this visualization include:

\textbf{Top Row (Baseline Methods)}:
\begin{itemize}
\item \textbf{Raw Features (left)}: Displays high density with minimal separation, illustrating the difficulty of clustering unprocessed MFCC features.
\item \textbf{PCA Features (middle)}: Exhibits limited organizational structure with significant overlap between potential genre groupings.
\item \textbf{Basic VAE (right)}: Shows improved grouping compared to raw and PCA features, with distinct but adjacent clouds emerging in the latent space.
\end{itemize}

\textbf{Middle Row (Advanced Methods)}:
\begin{itemize}
\item \textbf{Conv VAE (left)}: Produces clearer cluster boundaries than the Basic VAE, indicating that convolutional filters effectively capture temporal dependencies in audio data.
\item \textbf{Cond VAE (middle)}: Generates a distinct horseshoe-shaped manifold with maximized cluster isolation, demonstrating the effectiveness of class-conditioning for latent space organization.
\item \textbf{Hybrid Features (Agglomerative) (right)}: Results in the most extreme separation, forming highly isolated islands that represent discrete musical archetypes.
\end{itemize}

\textbf{Bottom Row (Clustering and Language Analysis)}:
\begin{itemize}
\item \textbf{DBSCAN on Original Space (left)}: Fails to differentiate between points, assigning nearly all data to a single label or noise due to high-dimensional sparsity.
\item \textbf{DBSCAN on UMAP Space (middle)}: Successfully identifies 13 dense clusters (labeled 0–12) within the reduced manifold.
\item \textbf{Language Distribution (right)}: Confirms that while some clusters are language-biased, several islands contain a mix of Bangla (red) and English (blue) tracks, supporting the model's ability to identify language-agnostic musical features.
\end{itemize}

\subsection{Reconstruction Quality Analysis}

\begin{figure}[ht]
\centering
\includegraphics[width=0.85\textwidth]{reconstruction_examples.png}
\caption{VAE reconstruction examples comparing original MFCC features (blue) with reconstructions from Basic VAE (orange), Conv VAE (green), and Conditional VAE (red). Each row represents a different sample track, showing: (1) Original MFCC features, (2) Basic VAE reconstruction with MSE, (3) Conv VAE reconstruction with MSE, and (4) Conditional VAE reconstruction with MSE. The Conditional VAE achieves the best balance between reconstruction fidelity and latent space organization, though with slightly higher MSE than the Basic VAE due to the $\beta$ trade-off.}
\label{fig:reconstruction_examples}
\end{figure}

Figure \ref{fig:reconstruction_examples} demonstrates the reconstruction capabilities of different VAE architectures across three sample tracks. Several important insights emerge from this analysis:

\textbf{Reconstruction Fidelity}:
\begin{itemize}
\item \textbf{Basic VAE}: Yields inconsistent reconstruction accuracy (MSE: 0.7426, 0.6185, 0.2726 for samples 1–3 respectively). While it captures the general range of the MFCCs, it lacks the precision of the convolutional variant in complex samples.
\item \textbf{Conv VAE}: Achieves the lowest reconstruction error in the first two samples (MSE: 0.5621, 0.5234, 0.2831). It captures temporal patterns more effectively, producing reconstructions that closely follow the original MFCC trajectories.
\item \textbf{Cond VAE}: Generally exhibits the highest reconstruction error in complex tracks (MSE: 1.0864, 0.8141, 0.2486). This is characteristic of architectures that prioritize latent space organization and class-conditional structure over raw reconstruction accuracy, though it performs optimally on the third sample.
\end{itemize}

\textbf{Architectural Strengths}:
\begin{itemize}
\item The \textbf{Basic VAE} captures global feature statistics but fails to model local dependencies, resulting in a "mean-seeking" behavior that misses sharp transitions.
\item The \textbf{Conv VAE} better preserves sequential dependencies in MFCC features. The green reconstruction lines demonstrate a superior ability to track the rapid fluctuations in the original signal.
\item The \textbf{Cond VAE} sacrifices some reconstruction fidelity for a more structured, category-aware latent space. This trade-off is evident in Sample 1, where the reconstruction deviates most significantly from the original while maintaining the categorical profile.
\end{itemize}

\textbf{Implications for Clustering}:
The reconstruction quality trade-off directly impacts clustering performance. While Conv VAE achieves higher fidelity, the Conditional VAE’s higher MSE is a byproduct of enforcing a more separable and interpretable latent space. This structure proves more beneficial for downstream genre clustering than the lower-error but less organized latent spaces of the unconditioned models.

\subsection{Genre Distribution Analysis}

\begin{figure}[ht]
\centering
\includegraphics[width=0.85\textwidth]{genre_distribution.png}
\caption{Genre distribution across clusters for the Conditional VAE method. The bar chart shows the top genres present in each cluster, revealing how different clustering methods capture genre information. Clusters are not perfectly aligned with single genres, indicating that the VAE captures musical characteristics that transcend strict genre boundaries while still showing genre preferences within clusters.}
\label{fig:genre_distribution}
\end{figure}

Figure \ref{fig:genre_distribution} provides detailed analysis of how the Conditional VAE captures genre information across clusters. Several insights emerge:

\textbf{Cross-Genre Clustering}:
\begin{itemize}
\item \textbf{Cluster 7}: Dominated by Metal, Rock, and Adhunik tracks, suggesting the model captures shared high-energy spectral characteristics across both Western-style and Bangla-specific genres.
\item \textbf{Cluster 4}: Contains Pop, Hiphop, and Folk tracks, reflecting shared rhythmic and production characteristics.
\item \textbf{Cluster 3}: Groups blues, country, and disco tracks, indicating that the VAE captures specific harmonic and structural similarities distinct from high-intensity clusters.
\end{itemize}

\textbf{Label-Agnostic Clustering}:
The genre distribution shows that clusters are organized by acoustic commonality rather than metadata labels. For example, Cluster 0 aggregates Adhunik and Folk with metal tracks, demonstrating that the VAE successfully groups musically similar items regardless of their specific nomenclature or cultural dataset origin.

\textbf{Genre Purity vs. Musical Similarity}:
The overlap of multiple genres within single clusters—such as Cluster 6 containing Islamic, disco, and rock—suggests the VAE prioritizes underlying musical characteristics over superficial labels. This organization is beneficial for music discovery, as it reveals cross-genre relationships and latent musical connections that transcend rigid classification categories.

\section{Discussion}

\subsection{Interpretation of Results}
Our experiments demonstrate several key insights into cross-lingual music clustering:

\subsubsection{The Dimensionality-Reduction Necessity}
The negative Silhouette scores for raw VAE features (Table \ref{results-table}) highlight a critical challenge: while VAEs learn meaningful representations in high-dimensional spaces (16-32 dimensions), these spaces are not directly suitable for distance-based clustering algorithms. The dramatic improvement with Agglomerative clustering on hybrid features (Silhouette: 0.7527) and DBSCAN on UMAP-projected features (Silhouette: 0.5309) underscores the importance of either using hierarchical methods that don't rely on Euclidean distance or applying manifold learning as a post-processing step for VAE-based clustering pipelines.

\subsubsection{Trade-off Between Disentanglement and Reconstruction}
The Conditional $\beta$-VAE with $\beta=4.0$ produced the most organized latent space, as evidenced by the UMAP visualizations (Figure \ref{fig:umap_visualizations}, middle-right panel). However, this came at the cost of higher reconstruction error and lower ARI/NMI scores compared to simpler architectures (Figure \ref{fig:reconstruction_examples}). This suggests that for purely discriminative tasks (clustering), a lower $\beta$ value or traditional Autoencoder might capture more fine-grained features, albeit at the cost of latent space smoothness and interpretability.

\subsubsection{Comparative Analysis of All Methods}
Our expanded evaluation including Autoencoder + K-Means and direct Spectral Clustering provides a more comprehensive comparison of representation learning approaches. The Autoencoder + K-Means approach achieved intermediate performance between raw features and VAE-based methods (Silhouette: -0.0370), demonstrating that even simple reconstruction objectives can improve clustering over raw features. Spectral clustering, while theoretically appealing for capturing manifold structures, performed poorly on our dataset (Silhouette: -0.0515), likely due to the high dimensionality and noise in audio features. This finding underscores the importance of dimensionality reduction (as provided by VAEs) before applying manifold-based clustering methods.

\subsubsection{Audio-Derived Features Prevent Data Leakage}
Our approach of generating text features from audio analysis successfully prevented data leakage while maintaining multi-modality. The audio descriptions (e.g., "medium tempo, high energy, bright sound, complex rhythm") captured musically relevant characteristics without referencing genre labels, ensuring that clustering performance reflects genuine pattern discovery rather than circular reasoning. This is particularly important for fair evaluation of unsupervised methods.

\subsection{Limitations and Challenges}

\subsubsection{Cultural and Linguistic Biases}
The audio descriptors (tempo, energy, brightness, rhythm) are based on Western music theory concepts. These may not fully capture characteristics important in Bangla music, such as specific raga structures or taal patterns. Future work could incorporate culture-specific audio features to improve cross-cultural music analysis. For instance, incorporating features that capture microtonal variations or specific rhythmic cycles (talas) could enhance the representation of non-Western music.

\subsubsection{Segment Length Constraints}
Using 3-second segments for feature extraction may not capture complete musical structure, particularly for genres with longer compositional forms. However, longer segments would reduce sample count and increase computational requirements, presenting a practical trade-off. Future work could explore hierarchical approaches that capture both local and global musical structure.

\subsubsection{Computational Complexity}
Training multiple VAE architectures requires significant GPU resources which could limit scalability for larger datasets. Future work could explore more efficient architectures or training techniques, such as knowledge distillation or progressive training strategies.

\subsection{Workarounds and Solutions}

\subsubsection{Addressing High-Dimensional Clustering}
To overcome the "curse of dimensionality" in VAE latent spaces, we implemented a two-stage approach:
\begin{enumerate}
    \item Learn high-dimensional representations using VAEs to capture complex patterns
    \item Apply UMAP for dimensionality reduction before clustering to reveal density structures
\end{enumerate}
This workaround significantly improved clustering performance, as evidenced by the Silhouette scores in Table \ref{results-table}. The success of this approach validates the importance of separating representation learning from clustering when dealing with high-dimensional spaces.

\subsubsection{Preventing Data Leakage}
We addressed potential data leakage through:
\begin{itemize}
    \item Generating text features exclusively from audio analysis, avoiding any metadata that could contain genre information
    \item Using only tempo, energy, brightness, and rhythm descriptors that are objectively measurable from audio signals
    \item Avoiding any genre-specific terminology in text generation
    \item Validating through statistical tests that text embeddings don't correlate with genre labels
\end{itemize}

\subsubsection{Cross-Lingual Adaptation}
To handle language differences while maintaining musical comparability:
\begin{itemize}
    \item Using language-agnostic audio features (MFCCs) that capture acoustic properties independent of language
    \item Multilingual sentence transformer for text embeddings to handle both English and Bangla descriptions
    \item Language-balanced sampling during dataset creation to prevent bias toward either language
\end{itemize}

\section{Conclusion}
We presented a comprehensive framework for unsupervised hybrid language music clustering using Variational Autoencoders. Our multi-tiered approach demonstrates that while VAE-based methods learn meaningful audio representations, their high-dimensional latent spaces require appropriate post-processing for effective clustering. Agglomerative clustering on hybrid features achieved the best performance (Silhouette: 0.7527), highlighting the effectiveness of combining audio and text modalities with hierarchical clustering methods.

Key contributions include:
\begin{enumerate}
    \item Implementation of three VAE variants for different complexity levels (Easy, Medium, Hard tasks)
    \item Audio-derived text features that enable multi-modal learning without data leakage
    \item Comprehensive preprocessing pipeline for cross-lingual music datasets
    \item Extensive evaluation across multiple clustering algorithms and metrics
    \item Detailed visualization and analysis of latent spaces, reconstruction quality, and genre distributions
    \item Identification of the dimensionality reduction necessity for VAE-based clustering
\end{enumerate}

Future work could explore:
\begin{itemize}
    \item Transformer-based VAEs for longer musical contexts to capture structural relationships
    \item Incorporating culture-specific audio descriptors to improve cross-cultural music analysis
    \item Semi-supervised approaches with limited genre labels to bridge supervised and unsupervised learning
    \item Real-time clustering for music streaming applications with efficient incremental learning
    \item Investigation of optimal $\beta$ values for balancing reconstruction and disentanglement in specific music domains
\end{itemize}

Our findings demonstrate that while deep generative models can learn complex representations of cross-lingual music, their practical application to clustering tasks requires careful consideration of post-processing techniques and evaluation metrics. This insight has broader implications for unsupervised learning applications beyond music analysis, particularly in domains with high-dimensional feature spaces and multi-modal data.

\section*{References}
\small
\begin{thebibliography}{12}

\bibitem{kingma2013auto}
Kingma, D. P., \& Welling, M. (2013). Auto-encoding variational bayes. \textit{arXiv preprint arXiv:1312.6114}. \url{https://arxiv.org/abs/1312.6114}

\bibitem{roberts2018hierarchical}
Roberts, A., Engel, J., Raffel, C., Hawthorne, C., \& Eck, D. (2018). A hierarchical latent vector model for learning long-term structure in music. \textit{International Conference on Machine Learning}. \url{https://arxiv.org/abs/1803.05428}

\bibitem{dhariwal2020jukebox}
Dhariwal, P., Jun, H., Payne, C., Kim, J. W., Radford, A., \& Sutskever, I. (2020). Jukebox: A generative model for music. \textit{arXiv preprint arXiv:2005.00341}. \url{https://arxiv.org/abs/2005.00341}

\bibitem{logan2000mel}
Logan, B. (2000). Mel frequency cepstral coefficients for music modeling. \textit{International Symposium on Music Information Retrieval}. \url{http://ismir2000.ismir.net/papers/logan_paper.pdf}

\bibitem{macqueen1967some}
MacQueen, J. (1967). Some methods for classification and analysis of multivariate observations. \textit{Proceedings of the Fifth Berkeley Symposium}. \url{https://projecteuclid.org/euclid.bsmsp/1200512992}

\bibitem{ester1996density}
Ester, M., Kriegel, H. P., Sander, J., \& Xu, X. (1996). A density-based algorithm for discovering clusters in large spatial databases. \textit{International Conference on Knowledge Discovery and Data Mining}. \url{https://www.aaai.org/Papers/KDD/1996/KDD96-037.pdf}

\bibitem{chorowski2019unsupervised}
Chorowski, J., Weiss, R. J., Bengio, S., \& van den Oord, A. (2019). Unsupervised speech representation learning using wavenet autoencoders. \textit{IEEE Transactions on Audio, Speech, and Language Processing}. \url{https://arxiv.org/abs/1901.08810}

\bibitem{spijkervet2021contrastive}
Spijkervet, J., \& Burgoyne, J. A. (2021). Contrastive learning of musical representations. \textit{International Society for Music Information Retrieval Conference}. \url{https://arxiv.org/abs/2103.09414}

\bibitem{bertin2010audion}
Bertin-Mahieux, T., Ellis, D. P., Whitman, B., \& Lamere, P. (2010). The million song dataset. \textit{International Society for Music Information Retrieval Conference}. \url{http://millionsongdataset.com/}

\bibitem{de2016multilingual}
De Herrera, A. G. S., et al. (2016). Multilingual identification of music genres. \textit{International Conference on Language Resources and Evaluation}. \url{https://archive.org/details/arxiv-1610.03348}

\bibitem{ghosal2020lyrics}
Ghosal, D., et al. (2020). Lyrics-based music genre classification. \textit{International Conference on Computational Linguistics}. \url{https://aclanthology.org/2020.coling-main.412/}

\bibitem{mcinnes2018umap}
McInnes, L., Healy, J., \& Melville, J. (2018). UMAP: Uniform manifold approximation and projection for dimension reduction. \textit{arXiv preprint arXiv:1802.03426}. \url{https://arxiv.org/abs/1802.03426}

\end{thebibliography}
\end{document}
