# VAE for Hybrid Language Music Clustering

Unsupervised learning pipeline for clustering English and Bangla music tracks using Variational Autoencoders (VAEs). This project implements three VAE architectures with multi-modal audio-text features and evaluates clustering performance across multiple algorithms and metrics.

## ğŸ“‹ Project Overview

This project addresses the challenge of cross-lingual music clustering by implementing a progressive VAE framework:
- **Easy Task**: Basic VAE for audio feature extraction + K-Means
- **Medium Task**: Convolutional VAE + hybrid features + advanced clustering
- **Hard Task**: Conditional Beta-VAE + multi-modal clustering with disentanglement

## ğŸ—ï¸ Architecture

### VAE Variants
1. **Basic VAE**: Linear encoder-decoder with 16D latent space
2. **Conv VAE**: 1D convolutional architecture for MFCC sequences
3. **Conditional Beta-VAE**: Genre-conditioned with Î²=4.0 for disentanglement

### Feature Engineering
- **Audio**: 13 MFCC coefficients with statistical moments (52D)
- **Text**: Audio-derived descriptions embedded via multilingual SentenceTransformer (384D)
- **Hybrid**: Concatenated audio + text features (436D)

### Clustering Methods
- K-Means (baseline)
- Agglomerative Clustering
- DBSCAN (original + UMAP space)
- Spectral Clustering
- PCA + K-Means

## ğŸ“Š Results

### Key Metrics (Top Performers)
| Method | Task | Silhouette | ARI | NMI | Purity |
|--------|------|------------|-----|-----|--------|
| Agglomerative | Hard | 0.7527 | 0.0043 | 0.0331 | 0.0945 |
| Hybrid K-Means | Medium | 0.7479 | 0.0042 | 0.0345 | 0.0960 |
| DBSCAN (UMAP) | Medium | 0.5309 | 0.0218 | 0.1061 | 0.1311 |
| Cond VAE | Hard | -0.0463 | 0.0749 | 0.1742 | 0.1761 |

### Key Findings
1. VAE latent spaces require UMAP projection for effective clustering
2. Audio-derived text features prevent data leakage from genre labels
3. Agglomerative clustering on hybrid features performs best
4. Conditional VAE achieves best latent space organization but lower reconstruction

## ğŸ—‚ï¸ Repository Structure
VAE-for-Hybrid-Language-Music-Clustering/
â”œâ”€â”€ data/
â”‚ â”œâ”€â”€ raw/ # (Empty - datasets downloaded via kagglehub)
â”‚ â””â”€â”€ processed/ # Processed datasets and features
â”œâ”€â”€ notebooks/
â”‚ â””â”€â”€ CSE425_Project_Final.ipynb # Complete implementation notebook
â”œâ”€â”€ src/
â”‚ â”œâ”€â”€ data_loader.py # Dataset loading and merging
â”‚ â”œâ”€â”€ feature_extractor.py # MFCC + text feature extraction
â”‚ â”œâ”€â”€ vae_models.py # VAE architecture definitions
â”‚ â”œâ”€â”€ clustering.py # Clustering algorithms
â”‚ â””â”€â”€ evaluation.py # Metrics calculation
â”œâ”€â”€ models/ # Trained model weights (.pth files)
â”œâ”€â”€ results/
â”‚ â”œâ”€â”€ metrics/ # Evaluation metrics in CSV
â”‚ â””â”€â”€ plots/ # All visualizations (UMAP, reconstructions, etc.)
â”œâ”€â”€ requirements.txt # Python dependencies
â””â”€â”€ README.md # This file

text

## ğŸš€ Quick Start

### Installation
```bash
git clone https://github.com/wasi4real/VAE-for-Hybrid-Language-Music-Clustering.git
cd VAE-for-Hybrid-Language-Music-Clustering
pip install -r requirements.txt
Run Complete Pipeline
python
# From notebook:
notebooks/CSE425_Project_Final.ipynb

# Or from Python modules:
python src/data_loader.py      # Load and merge datasets
python src/feature_extractor.py # Extract features
python src/vae_models.py       # Train VAEs
python src/clustering.py       # Apply clustering
python src/evaluation.py       # Calculate metrics
Requirements
See requirements.txt for complete list. Key packages:

torch, librosa, scikit-learn, umap-learn

sentence-transformers, pandas, matplotlib

ğŸ“ˆ Visualizations
Generated Plots
umap_visualizations.png - UMAP projections of all feature spaces

reconstruction_examples.png - VAE reconstruction comparisons

genre_distribution.png - Cluster composition by genre

All plots available in results/plots/

ğŸ“ Dataset Information
Sources
GTZAN: 1,000 English tracks across 10 genres (100 per genre)

BanglaBeats: 16,170 Bangla tracks across 8 genres

Preprocessing
Balanced sampling: 1,000 tracks from each language

Audio: 3-second segments, 22050Hz, MFCC extraction

Text: Audio-derived descriptions (tempo, energy, brightness, rhythm)

Features: StandardScaler normalization

ğŸ¯ Project Tasks Completed
Easy Task
âœ“ Basic VAE implementation

âœ“ K-Means clustering on latent features

âœ“ UMAP visualization

âœ“ Comparison with PCA + K-Means baseline

Medium Task
âœ“ Convolutional VAE architecture

âœ“ Hybrid audio-text features

âœ“ Multiple clustering algorithms (K-Means, Agglomerative, DBSCAN)

âœ“ Davies-Bouldin Index evaluation

Hard Task
âœ“ Conditional Beta-VAE (Î²=4.0)

âœ“ Multi-modal clustering with audio + text

âœ“ All metrics: Silhouette, ARI, NMI, Cluster Purity

âœ“ Latent space visualizations and disentanglement analysis

ğŸ› ï¸ Technical Details
Data Leakage Prevention
Text features derived from audio analysis only

No genre information in feature generation

Genre labels used only for conditioning (CVAE) and evaluation

Workarounds Implemented
UMAP projection for high-dimensional clustering

Audio-derived text features for multi-modality

Balanced language sampling for fair evaluation

ğŸ“š References
Kingma & Welling (2013). Auto-Encoding Variational Bayes

Logan (2000). Mel Frequency Cepstral Coefficients for music modeling

McInnes et al. (2018). UMAP: Uniform Manifold Approximation and Projection

ğŸ‘¤ Author
Moin Mostakim
Neural Networks Course Project (CSE425)

ğŸ“„ License
This project is for academic purposes as part of CSE425 coursework.

text
